{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e41d6d29-d1ad-47f2-8050-fb7c2e2890cc",
   "metadata": {},
   "source": [
    "Q1. Install and load the latest versions of TensorFlow and Keras. Print their versions. \n",
    "\n",
    "Q2. Load the Wine Quality dataset and explore its dimensions. \n",
    "\n",
    "Dataset link:  \n",
    "https://www.kaggle.com/datasets/nareshbhat/wine-quality-binary-classification \n",
    "\n",
    "Q3. Check for null values, identify categorical variables, and encode them. \n",
    "\n",
    "Q4. Separate the features and target variables from the dataframe. \n",
    "\n",
    "Q5. Perform a train-test split and divide the data into training, validation, and test datasets. Q6. Perform scaling on the dataset. \n",
    "\n",
    "Q7. Create at least 2 hidden layers and an output layer for the binary categorical variables. Q8. Create a Sequential model and add all the \n",
    "layers to it. \n",
    "\n",
    "Q9. Implement a TensorBoard callback to visualize and monitor the model's training process. \n",
    "\n",
    "Q10. Use Early Stopping to prevent overfitting by monitoring a chosen metric and stopping the training if  no improvement is observed. \n",
    "\n",
    "Q11. Implement a ModelCheckpoint callback to save the best model based on a chosen metric during  training. \n",
    "\n",
    "Q12. Print the model summary. \n",
    "\n",
    "Q13. Use binary cross-entropy as the loss function, Adam optimizer, and include the metric ['accuracy']. Q14. Compile the model with the \n",
    "specified loss function, optimizer, and metrics. \n",
    "\n",
    "Q15. Fit the model to the data, incorporating the TensorBoard, Early Stopping, and ModelCheckpoint  callbacks. \n",
    "\n",
    "Q16. Get the model's parameters. \n",
    "\n",
    "Q17. Store the model's training history as a Pandas DataFrame. \n",
    "\n",
    "Q18. Plot the model's training history. \n",
    "\n",
    "Q19. Evaluate the model's performance using the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bdcb0f-7611-490d-aef0-9f511ab94808",
   "metadata": {},
   "source": [
    "Q1. Install and load the latest versions of TensorFlow and Keras:\n",
    "\n",
    "You can install the latest versions of TensorFlow and Keras using pip:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "```\n",
    "\n",
    "In your Python script or Jupyter Notebook, you can load the libraries as follows:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "```\n",
    "\n",
    "Q2. Load the Wine Quality dataset and explore its dimensions:\n",
    "\n",
    "Download the Wine Quality dataset from the provided link and load it into a Pandas DataFrame:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the dataset is stored in a CSV file named wine_quality.csv\n",
    "data = pd.read_csv('wine_quality.csv')\n",
    "\n",
    "# Explore the dimensions of the dataset\n",
    "print(data.shape)\n",
    "```\n",
    "\n",
    "Q3. Check for null values, identify categorical variables, and encode them:\n",
    "\n",
    "```python\n",
    "# Check for null values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Identify categorical variables (if any)\n",
    "categorical_vars = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Perform one-hot encoding on categorical variables (if any)\n",
    "if len(categorical_vars) > 0:\n",
    "    data = pd.get_dummies(data, columns=categorical_vars, drop_first=True)\n",
    "```\n",
    "\n",
    "Q4. Separate the features and target variables from the DataFrame:\n",
    "\n",
    "```python\n",
    "X = data.drop('target_column_name', axis=1)  # Replace 'target_column_name' with the actual target column\n",
    "y = data['target_column_name']  # Replace 'target_column_name' with the actual target column\n",
    "```\n",
    "\n",
    "Q5. Perform a train-test split and divide the data into training, validation, and test datasets:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training data into training and validation sets (80% training, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "Q6. Perform scaling on the dataset:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform the training, validation, and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "Q7. Create at least 2 hidden layers and an output layer for the binary categorical variables:\n",
    "\n",
    "```python\n",
    "# Assuming you have the number of input features as 'input_dim'\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "\n",
    "# Create the Sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Add the hidden layers\n",
    "model.add(keras.layers.Dense(64, activation='relu', input_dim=input_dim))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # For binary classification\n",
    "```\n",
    "\n",
    "Q8. Create a Sequential model and add all the layers to it:\n",
    "\n",
    "The code in Q7 already does this.\n",
    "\n",
    "Q9. Implement a TensorBoard callback to visualize and monitor the model's training process:\n",
    "\n",
    "```python\n",
    "# Define the TensorBoard callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs/', histogram_freq=1)\n",
    "\n",
    "# Add the callback to the fit function in Q15\n",
    "```\n",
    "\n",
    "Q10. Use Early Stopping to prevent overfitting by monitoring a chosen metric and stopping the training if no improvement is observed:\n",
    "\n",
    "```python\n",
    "# Define the Early Stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Add the callback to the fit function in Q15\n",
    "```\n",
    "\n",
    "Q11. Implement a ModelCheckpoint callback to save the best model based on a chosen metric during training:\n",
    "\n",
    "```python\n",
    "# Define the ModelCheckpoint callback\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Add the callback to the fit function in Q15\n",
    "```\n",
    "\n",
    "Q12. Print the model summary:\n",
    "\n",
    "```python\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "Q13. Use binary cross-entropy as the loss function, Adam optimizer, and include the metric ['accuracy']:\n",
    "\n",
    "```python\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Q14. Compile the model with the specified loss function, optimizer, and metrics:\n",
    "\n",
    "The code in Q13 already does this.\n",
    "\n",
    "Q15. Fit the model to the data, incorporating the TensorBoard, Early Stopping, and ModelCheckpoint callbacks:\n",
    "\n",
    "```python\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_val_scaled, y_val),\n",
    "                    callbacks=[tensorboard_callback, early_stopping_callback, model_checkpoint_callback])\n",
    "```\n",
    "\n",
    "Q16. Get the model's parameters:\n",
    "\n",
    "```python\n",
    "model_params = model.get_weights()\n",
    "```\n",
    "\n",
    "Q17. Store the model's training history as a Pandas DataFrame:\n",
    "\n",
    "```python\n",
    "history_df = pd.DataFrame(history.history)\n",
    "```\n",
    "\n",
    "Q18. Plot the model's training history:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_df['loss'], label='Training Loss')\n",
    "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history_df['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Q19. Evaluate the model's performance using the test data:\n",
    "\n",
    "```python\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88e34c-44b9-49d8-bedb-69bfa76f62bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
